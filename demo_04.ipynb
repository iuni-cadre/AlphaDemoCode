{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) \n",
      "[GCC 7.3.0]\n",
      "/home/jovyan\n",
      "/home/jovyan/conda\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(sys.version)\n",
    "print(os.environ['HOME'])\n",
    "java_path = '/home/jovyan/conda'\n",
    "os.environ['JAVA_HOME'] = java_path\n",
    "print(os.environ.get('JAVA_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51d7ede7641b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda/dbconnect/lib/python3.5/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/dbconnect/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/dbconnect/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/conda/dbconnect/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/dbconnect/lib/python3.5/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \"\"\"\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_launch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/dbconnect/lib/python3.5/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36m_launch_gateway\u001b[0;34m(conf, insecure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/AWStest/', name='AWStest/', size=0),\n",
       " FileInfo(path='dbfs:/mnt/WoS/', name='WoS/', size=0),\n",
       " FileInfo(path='dbfs:/mnt/test/', name='test/', size=0),\n",
       " FileInfo(path='dbfs:/mnt/test2/', name='test2/', size=0),\n",
       " FileInfo(path='dbfs:/mnt/test3/', name='test3/', size=0),\n",
       " FileInfo(path='dbfs:/mnt/test7/', name='test7/', size=0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "dbutils = DBUtils(spark.sparkContext)\n",
    "dbutils.fs.ls(\"/mnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ego = pd.read_csv('../../packages/issi_data_package/output_files/data/774e7eb6-6ac7-4dd5-9339-531b746cb8ec.csv')\n",
    "degree1 = pd.read_csv('../../packages/issi_data_package/output_files/data/774e7eb6-6ac7-4dd5-9339-531b746cb8ec_nodes.csv')\n",
    "#display(ego,degree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "From                    105242\n",
       "To                      105242\n",
       "paper_id                105242\n",
       "year                    105242\n",
       "original_title          105242\n",
       "authors_display_name    105242\n",
       "journal_display_name    105242\n",
       "paper_abstract          103151\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = pd.read_csv('../../packages/issi_data_package/output_files/data/a71d68fa-a479-47b6-8e55-ce8f12eacb56_edges.csv')\n",
    "citations = pd.merge(edges, ego, left_on=['To'], right_on=['paper_id'])\n",
    "citations.count()\n",
    "#edges.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "focal                   4027234\n",
       "connector               4027234\n",
       "From                    4027234\n",
       "To                      4027234\n",
       "paper_id                4027234\n",
       "year                    4027234\n",
       "original_title          4027234\n",
       "authors_display_name    4027234\n",
       "journal_display_name    4027234\n",
       "paper_abstract          3964481\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations0 = citations[['From','To']]\n",
    "citations0.columns = ['focal', 'connector']\n",
    "citations2 = pd.merge(citations0, citations, left_on=['connector'], right_on=['To'])\n",
    "citations2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "focal                   6657184\n",
       "connector               6657184\n",
       "From                    6657184\n",
       "To                      6657184\n",
       "paper_id                6657184\n",
       "year                    6657184\n",
       "original_title          6657184\n",
       "authors_display_name    6657184\n",
       "journal_display_name    6657184\n",
       "paper_abstract          6562803\n",
       "cited2                  6657184\n",
       "LE                      6657184\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations00 = citations0\n",
    "citations00.columns = ['cited2','LE']\n",
    "citations3 = pd.merge(citations2, citations00, left_on=['focal','To'], right_on=['cited2','LE'])\n",
    "citations3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct citations\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "paper_citing = defaultdict(list)\n",
    "\n",
    "citingFile = open(\"../../packages/issi_data_package/output_files/data/a71d68fa-a479-47b6-8e55-ce8f12eacb56_edges.csv\", \"r\")\n",
    "citingFile.readline()\n",
    "for line in citingFile:\n",
    "    line = line.strip().replace('\"', '').split(\",\")\n",
    "    paper_citing[int(line[1])].append(int(line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.025060772373004536 of total focal papers...\n",
      "\n",
      "Processed 0.05012154474600907 of total focal papers...\n",
      "\n",
      "Processed 0.0751823171190136 of total focal papers...\n",
      "\n",
      "Processed 0.10024308949201814 of total focal papers...\n",
      "\n",
      "Processed 0.12530386186502268 of total focal papers...\n",
      "\n",
      "Processed 0.1503646342380272 of total focal papers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# indirect citations\n",
    "count = 0\n",
    "total = len(degree1)\n",
    "indirect_citation = defaultdict(list)\n",
    "paperList = ego['paper_id'].tolist()\n",
    "\n",
    "for paper in paperList:\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"Processed \" + str(float(count) / float(total)) + \" of total focal papers...\\n\")\n",
    "    for citing_paper_1 in paper_citing[paper]:\n",
    "        for citing_paper_2 in paper_citing[paper]:\n",
    "            if citing_paper_1 in paper_citing[citing_paper_2]:\n",
    "                temp = []\n",
    "                temp.append(citing_paper_1)\n",
    "                temp.append(citing_paper_2)\n",
    "                indirect_citation[paper].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+------+------+------+---+------+------+------+------+----+----+------+------+------+------+------+------+------+------+------+\n",
      "|summary|_c0   |_c1   |_c2   |_c3   |_c4   |_c5   |_c6|_c7   |_c8   |_c9   |_c10  |_c11|_c12|_c13  |_c14  |_c15  |_c16  |_c17  |_c18  |_c19  |_c20  |_c21  |\n",
      "+-------+------+------+------+------+------+------+---+------+------+------+------+----+----+------+------+------+------+------+------+------+------+------+\n",
      "|count  |138953|138953|135508|138953|138953|138953|25 |138953|138953|135368|138953|0   |0   |135531|135099|133690|127181|138953|138953|138953|138953|138953|\n",
      "+-------+------+------+------+------+------+------+---+------+------+------+------+----+----+------+------+------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sql\n",
    "papers = spark.read.format(\"parquet\").load(\"/mnt/test/graph/2019-1-11-PNAS/papers/*.parquet\")\n",
    "papers.describe().filter(sql.col(\"summary\") == \"count\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please wait a moment while I gather a list of all available modules...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/dbconnect/lib/python3.5/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead.\n",
      "  \"You should import from ipykernel or jupyter_client instead.\", ShimWarning)\n",
      "/home/jovyan/conda/dbconnect/lib/python3.5/pkgutil.py:104: VisibleDeprecationWarning: zmq.eventloop.minitornado is deprecated in pyzmq 14.0 and will be removed.\n",
      "    Install tornado itself to use zmq with the tornado IOLoop.\n",
      "    \n",
      "  yield from walk_packages(path, name+'.', onerror)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDROM               autoreload          jsonschema          select\n",
      "DLFCN               backcall            jupyter             selectors\n",
      "IN                  base64              jupyter_client      send2trash\n",
      "IPython             bdb                 jupyter_core        setuptools\n",
      "PyQt5               binascii            keyword             shelve\n",
      "TYPES               binhex              kiwisolver          shlex\n",
      "__future__          bisect              lib2to3             shutil\n",
      "_ast                bleach              linecache           signal\n",
      "_bisect             builtins            locale              simplegeneric\n",
      "_bootlocale         bz2                 logging             sip\n",
      "_bz2                cProfile            lzma                sipconfig\n",
      "_codecs             calendar            macpath             sipdistutils\n",
      "_codecs_cn          certifi             macurl2path         site\n",
      "_codecs_hk          cgi                 mailbox             six\n",
      "_codecs_iso2022     cgitb               mailcap             smtpd\n",
      "_codecs_jp          chunk               markupsafe          smtplib\n",
      "_codecs_kr          cmath               marshal             sndhdr\n",
      "_codecs_tw          cmd                 math                socket\n",
      "_collections        code                matplotlib          socketserver\n",
      "_collections_abc    codecs              mimetypes           spwd\n",
      "_compat_pickle      codeop              mistune             sqlite3\n",
      "_compression        collections         mmap                sre_compile\n",
      "_crypt              colorsys            modulefinder        sre_constants\n",
      "_csv                compileall          multiprocessing     sre_parse\n",
      "_ctypes             concurrent          nb_conda            ssl\n",
      "_ctypes_test        configparser        nb_conda_kernels    stat\n",
      "_curses             contextlib          nbconvert           statistics\n",
      "_curses_panel       copy                nbformat            storemagic\n",
      "_datetime           copyreg             netrc               string\n",
      "_decimal            crypt               nis                 stringprep\n",
      "_dummy_thread       csv                 nntplib             struct\n",
      "_elementtree        ctypes              notebook            subprocess\n",
      "_functools          curses              ntpath              sunau\n",
      "_hashlib            cycler              nturl2path          symbol\n",
      "_heapq              cythonmagic         numbers             sympyprinting\n",
      "_imp                datetime            numpy               symtable\n",
      "_io                 dateutil            opcode              sys\n",
      "_json               dbm                 operator            sysconfig\n",
      "_locale             decimal             optparse            syslog\n",
      "_lsprof             decorator           os                  tabnanny\n",
      "_lzma               defusedxml          ossaudiodev         tarfile\n",
      "_markupbase         difflib             pandas              telnetlib\n",
      "_md5                dis                 pandocfilters       tempfile\n",
      "_multibytecodec     distutils           parser              terminado\n",
      "_multiprocessing    doctest             parso               termios\n",
      "_opcode             dummy_threading     pathlib             testpath\n",
      "_operator           easy_install        pdb                 tests\n",
      "_osx_support        email               pexpect             textwrap\n",
      "_pickle             encodings           pickle              this\n",
      "_posixsubprocess    ensurepip           pickleshare         threading\n",
      "_pydecimal          entrypoints         pickletools         time\n",
      "_pyio               enum                pip                 timeit\n",
      "_random             errno               pipes               tkinter\n",
      "_sha1               faulthandler        pkg_resources       token\n",
      "_sha256             fcntl               pkgutil             tokenize\n",
      "_sha512             filecmp             platform            tornado\n",
      "_signal             fileinput           plistlib            trace\n",
      "_sitebuiltins       fnmatch             poplib              traceback\n",
      "_socket             formatter           posix               tracemalloc\n",
      "_sqlite3            fractions           posixpath           traitlets\n",
      "_sre                ftplib              pprint              tty\n",
      "_ssl                functools           profile             turtle\n",
      "_stat               gc                  prometheus_client   turtledemo\n",
      "_string             genericpath         prompt_toolkit      types\n",
      "_strptime           getopt              pstats              typing\n",
      "_struct             getpass             pty                 unicodedata\n",
      "_symtable           gettext             ptyprocess          unittest\n",
      "_sysconfigdata      glob                pwd                 urllib\n",
      "_testbuffer         grp                 py4j                uu\n",
      "_testcapi           gzip                py_compile          uuid\n",
      "_testimportmultiple hashlib             pyclbr              venv\n",
      "_testmultiphase     heapq               pydoc               warnings\n",
      "_thread             hmac                pydoc_data          wave\n",
      "_threading_local    html                pyexpat             wcwidth\n",
      "_tkinter            http                pygments            weakref\n",
      "_tracemalloc        idlelib             pylab               webbrowser\n",
      "_warnings           imaplib             pyparsing           webencodings\n",
      "_weakref            imghdr              pyspark             wheel\n",
      "_weakrefset         imp                 pytz                wsgiref\n",
      "abc                 importlib           queue               xdrlib\n",
      "aifc                inspect             quopri              xml\n",
      "antigravity         io                  random              xmlrpc\n",
      "argparse            ipaddress           re                  xxlimited\n",
      "array               ipykernel           readline            xxsubtype\n",
      "ast                 ipykernel_launcher  reprlib             zipapp\n",
      "asynchat            ipython_genutils    resource            zipfile\n",
      "asyncio             itertools           rlcompleter         zipimport\n",
      "asyncore            jedi                rmagic              zlib\n",
      "atexit              jinja2              runpy               zmq\n",
      "audioop             json                sched               \n",
      "\n",
      "Enter any module name to get more help.  Or, type \"modules spam\" to search\n",
      "for modules whose name or summary contain the string \"spam\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (databricks-backend)",
   "language": "python",
   "name": "databricks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
